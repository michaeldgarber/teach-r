---
title: "R for data wrangling 2: more dplyr and some tidycensus, mapview, and ggplot"
author: "Michael Garber, PhD MPH"
date: "Revised August 3rd, 2022"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In the last tutorial (<https://michaeldgarber.github.io/teach-r/1-dplyr-nyt-covid.html>), we learned the basics of using **dplyr** to wrangle a state-level dataset on COVID-19 cases and deaths. In this tutorial, we will build on that work to calculate cumulative and daily incidence per population. To grab population data, we'll load census data using **tidycensus**. We also introduce **sf**, which is the workhorse library for wrangling spatial data in R. We'll join the census data with our COVID-19 dataset and do some interactive mapping with **mapview** and finish with some **ggplot2** graphs. To that end, let's make sure we have the following packages installed:

```{r "mapviewspecial", eval=FALSE, include=FALSE, eval=TRUE, echo=FALSE}
#install.packages("remotes")
library(remotes)
#Only run if haven't run previou
#remotes::install_github("r-spatial/mapview") #I install the github version to get this demo to work, but the CRAN version otherwise. https://github.com/r-spatial/mapview/issues/312
#library(mapview)
#mapviewOptions(fgb = FALSE) #because https://github.com/r-spatial/mapview/issues/321
```

```{r installpackages, eval=FALSE, echo=FALSE, warning=FALSE, message=FALSE}
install.packages("tidyverse")
install.packages("tidycensus")
install.packages("mapview")
install.packages("viridis")
install.packages("scales")
```

```{r loadpackages, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidycensus)
library(mapview) #for interactive mapping
library(sf) #simple features for working with spatial (vector) data
library(viridis) #used to change color palettes
library(scales) #Used to help with the ggplot date axis below.
```

```{r "mapviewspecial2", eval=FALSE, include=FALSE, eval=TRUE, echo=FALSE}
#install.packages("remotes")
mapviewOptions(fgb = FALSE) #because https://github.com/r-spatial/mapview/issues/321
```

# Re-generate the NYT COVID-19 dataset we were working with before.

This simply repeats the code that we used in the previous tutorial to give us the same dataset we were working with.

Load the state-level data from the NYTimes GitHub repository the same way we did before.

```{r load NYT data, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
nyt_state_url = "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv"
nyt_state_data = nyt_state_url %>% 
  url() %>% 
  read_csv()
```

This code reproduces our previous data wrangling where we calculated the daily number of incident cases.

Let's also add a couple more variables that we may use later on (year, month, and week) using the convenient [lubridate](https://lubridate.tidyverse.org/) package. We use the `year()`, `month()`, and `week()` functions. Note these functions only work on date columns. Is the column called date formatted as a date? Yes.

```{r}
class(nyt_state_data$date)
```

Lubridate doesn't automatically load when the tidyverse is loaded, so let's load it:

```{r}
library(lubridate)
```

```{r generatedata, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
nyt_state_wrangle = nyt_state_data %>% 
  group_by(state) %>% 
  arrange(date) %>% 
  rename(
    cases_cumul = cases,
    deaths_cumul = deaths
  ) %>% 
  mutate(
    cases_cumul_day_before = lag(cases_cumul),
    cases_incident = cases_cumul - cases_cumul_day_before,
    
    deaths_cumul_day_before = lag(deaths_cumul),
    deaths_incident = deaths_cumul - deaths_cumul_day_before,
    
    year=year(date),
    month=month(date),
    week = week(date) #week of the year (i.e, 1-52)
  ) %>% 
  ungroup() %>% 
  arrange(state, date)
```

Did year, month, and week get created as expected?

```{r}
nyt_state_wrangle %>% 
  dplyr::select(date, year, month, week)
```

# Load state-level census data using tidycensus

**Tidycensus** is a convenience package that allows census data to be downloaded into R at various geographic scales (e.g., state, county, census tract). The online material on tidycensus has great examples: <https://walker-data.com/tidycensus/>.

Use of tidycensus requires a key to access the U.S. Census API, which can be obtained for free here: <https://api.census.gov/data/key_signup.html>.

![](images/census-api-key-landing.png){width="500"}

You will get a confirmation email with your key asking you to activate it, and then hopefully you see something like this:

![](images/census-api-key-success.png){width="500"}

The first time you use the key on your computer, enter this line of code, entering the key you received by email:

```{r censuskey, eval=FALSE, echo=TRUE, warning=FALSE, message=TRUE}
census_api_key("your_census_key_here", install=TRUE)
```

Tidycensus lets the user specify

-   the survey (American Community Survey or U.S. Census),
-   the corresponding time period for that survey, the target geographic resolution and extent,
-   the census variables to be downloaded, and
-   whether or not the returned file will include the corresponding spatial data for the geographic units.

## Brief introduction to sf

By specifying `geometry=TRUE`, an **sf object** will be returned representing the object for the chosen geography. If `geometry=FALSE`, then only the aspatial data will be returned. sf (or [simple feature](https://r-spatial.github.io/sf/articles/sf1.html)) objects have become the standard data structure for managing spatial data in R. We will cover sf objects more in a separate tutorial (link TBD). I would also encourage you to review the excellent and free online material on sf:

-   The main landing page maintained by the package author, Edzer Pebesma: <https://r-spatial.github.io/sf/>

-   This chapter by Lovelace, Nowosad, and Muenchow: <https://geocompr.robinlovelace.net/spatial-class.html>. In fact, I highly recommend their whole book: <https://geocompr.robinlovelace.net/>.

-   Here is another book written by an economist that I have found to be a great resource. The link to the chapter on sf, specifically, is here: <https://tmieno2.github.io/R-as-GIS-for-Economists/vector-basics.html>

One great aspect of `sf` objects--as compared with the sp package, the previous standard--is that spatial data can be handled as you would any other rectangular dataframe. An `sf` object is like a `data.frame` or `tibble` with an added column representing the geometry for that observation. The data structure of sf makes it straightforward to integrate spatial data into a typical data-wrangling workflow, during which you might add new variables, filter on a variable, or merge with additional data.

## Load state-level population data

Without further adieu, let's run the tidycensus function `get_acs()` to return state-level population data from ACS:

```{r get states, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}

#Run this to save the data locally for speed.
options(tigris_use_cache = TRUE) 

#One way I keep track of whether an object has geometry associated with it to add a _geo suffix to the name.
pop_by_state_geo =get_acs(
  year=2019,
  variables = "B01001_001", #total population
  geography = "state",
  geometry = TRUE #to grab geometry
)
```

A convenient way to search through the census variables is to use the `load_variables` function, as described here: <https://walker-data.com/tidycensus/articles/basic-usage.html>.

```{r loadcensusvars, eval=FALSE, echo=TRUE, warning=FALSE, message=TRUE}
vars_acs_2019 = load_variables(2019, "acs5", cache = TRUE)
View(vars_acs_2019)
```

Confirm the data are an sf object (simple feature) and explore the variable names.

```{r confirmstatus, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
class(pop_by_state_geo)
pop_by_state_geo
```

## A quick mapview

Another essential package in the **sf** workflow is **mapview** (<https://r-spatial.github.io/mapview/index.html>), which creates quick interactive maps. In just one line, we can interactively map the ACS state-level population data we just loaded.

The `zcol` argument colors the polygons (states, here) based on the value of the variable, "estimate", which corresponds to the state's population, as obtained from tidycensus. The default color palette for the visualization is [viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html).

```{r mapview, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
mapview(pop_by_state_geo, zcol = "estimate")
```

Note that mapview can also be used with the pipe, as we'll see below.

```{r, eval=FALSE, echo=TRUE, warning=FALSE}
pop_by_state_geo %>% #the object name and then the pipe
  mapview( #Note the "object" argument is implied by the pipe operator, so we don't need to type it within mapview after a pipe.
    zcol = "estimate" )#the column to be visualized
```

## Wrangle the census data further to prep for the join with the NYT state-level COVID-19 data.

Let's clean up the output from tidycensus to make it easier to link with the state-level COVID-19 data.

```{r nameconfirm1, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
names(pop_by_state_geo)
```

The variable corresponding to the state name is called 'NAME' in the tidycensus output. Rename it to 'state' so it can be more easily joined with the NYT data. Also note that there is a column with the variable name ('variable') and another column for its 'estimate'. It's formatted this way because the default output from tidycensus is long-form data. That is, we could have downloaded several variables for each state, and each variable-state combination would receive its own row. We can consolidate these fields by dropping the variable column and renaming 'estimate' to 'population'. Finally, because GEOID may vary depending on the geographic unit, let's explicitly remember it's a two-digit FIPS code and call it 'fips_2digit.'

```{r nameconfirm2, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
names(nyt_state_wrangle)
```

We can wrangle this **sf** object the same way we would a tibble. For example, here we use a pipe (`%>%`) and the **dplyr** verbs `rename()`, `select()`, and `mutate()`. Within the `select()` function, we use the - sign to drop variables from our dataset.

```{r wranglepop, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
pop_by_state_wrangle_geo = pop_by_state_geo %>%
  rename(
    fips_2digit = GEOID,
    state = NAME,
    population = estimate
  ) %>% 
  dplyr::select(-moe, -variable) %>% 
  #To simplify some of the maps below, create an indicator variable corresponding to the Continental 48.
  #Do the same type of operation using two different types of syntax, the  %in% operator and the or operator.
  mutate(
    continental_48 = case_when(
      state %in% c("Alaska", "Hawaii", "Northern Mariana Islands", "Guam", "Virgin Islands", "Puerto Rico") ~ 0,
      TRUE ~ 1)    
    ) 

names(pop_by_state_wrangle_geo)
```

## Use mapview to visualize population.

To confirm the population data are as we expect, let's map the state's populations using **mapview**. The pipe operator, `%>%`, can also be used before a `mapview()` call. In this code, the sf object, `pop_by_state_geo`, and all subsequent changes we make it to it, e.g., after having used `filter()` here, are "piped" into the `mapview()` function. Another advantage of the pipe when using mapview is you can visualize various subsets (or other variations) of your data without having to createa a new object if you don't intend to use that variation later.

The `zcol` argument specifies which variable to be visualized. As you may notice, [mapview defaults](https://r-spatial.github.io/mapview/articles/articles/mapview_02-advanced.html) to the viridis palette.

```{r mapview1, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
pop_by_state_wrangle_geo %>%
  filter(continental_48==1) %>%   #Limit to continental 48 so the default zoom is more zoomed in.
  mapview( 
    zcol = "population" ) #Note the object argument is implied by the pipe operator.
```

# Link the NYT COVID-19 data with the census data

Let's link the NYTimes COVID-19 data with the census data two ways.

We will first map an overall summary of cumulative incidence per state population. To do that, we will re-run some of our summary estimates from the [previous tutorial](https://rpubs.com/michaeldgarber/dplyr_1) to calculate the total number of cumulative cases. We will then link the census population data to the NYTimes data by state using `left_join`. We will calculate a rate (cumulative cases per population) and visualize it using mapview.

## Summarize NYT COVID-19 data by state and link with census data.

Recall, we went through several ways to calculate the cumulative number of cases by state in our previous code. Let's group by state and take the max value using `summarise()`. Using the pipe, we can do these steps without having to create intermediate datasets.

`left_join()` takes three arguments: the data frame (or tibble) on the left side (piped through here), the data frame it's merging with on the right side (`pop_by_state_wrangle_geo`), and the key variable on which the two tibbles will be merged, which is state. Note `left_join()` can also join on a vector of (two or more) variables if your key is represented by a combination of variables: `by = c("var1", "var2")`. Here we need just one key variable.

We use `st_as_sf()` to force the dataframe to be an sf object. We finally use `mutate()` to create new variables which calcualte the cumulative incidence of both cases and deaths per population.

```{r wranglenytlink, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
nyt_by_state_w_pop_geo = nyt_state_wrangle %>% 
  group_by(state) %>% 
  summarise(
    cases_cumul = max(cases_cumul, na.rm=TRUE),
    deaths_cumul = max(deaths_cumul, na.rm=TRUE)
    ) %>% 
  #Recall, after we use group_by and summarise, the data go from a very big dataset 
  #with a row for each day-state combination to a smaller dataset with just a row for every state.
  left_join(pop_by_state_wrangle_geo, by = "state") %>% #This contains the geometry
  st_as_sf() %>% 
  mutate(
    cases_cumul_per_pop = cases_cumul/population,
    deaths_cumul_per_pop = deaths_cumul/population
  )
```

### Visualize cumulative incidence per population using mapview

Use mapview to visualize.

```{r mapview_cases_perpop, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
nyt_by_state_w_pop_geo %>%
  #Limit to continental 48 so the default zoom is more zoomed in.
  filter(continental_48==1) %>% 
  mapview(
    zcol = "cases_cumul_per_pop")
```

The variable `cases_cumul_per_pop` is visualized, and it looks like North Dakota has had a particularly high incidence rate. Note that this measure should not be interpreted as a proportion, as the same person can be counted as a case multiple times. That is, this measure is no not the percentage of the population that has had Covid-19; that proportion would probably be lower. We have counts of cases, not person-level data, so we can't determine what proportion of people have been a case.

This measure may be easier to interpret if we were to split the data into year increments, so we could then calculate cases per person-year.

### Exercise 1: Calculate cases per person-year for each state.

**Exercise:** Calculate the number of incident cases per person-year for each state.

### Tinker with mapview's defaults.

Mapview's main raison d'etre is as a quick workflow check, but it can also be used to make more refined maps. One can, for example, change the label of the visualized variable in the legend (`layer.name=`) or change the color palette (`col.regions=`). See options here: <https://r-spatial.github.io/mapview/articles/articles/mapview_02-advanced.html>. In the below map, I made those changes and flipped the direction of the viridis color palette using the `direction` argument so that dark purple is high incidence and yellow is low. Check out the viridis color palettes here: <https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html>. B corresponds to the inferno palette: <https://rdrr.io/cran/scales/man/viridis_pal.html>.

The output is not bad, and I have used mapview in public-facing documents. Some people prefer tmap, though, for publication-ready maps, [**tmap**](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html). We will discuss tmap more in a subsequent sesssion. (Also see this tmap demo by my colleague, Lance Owen: <https://rpubs.com/lancerowen/ElegantCartography_1>).

```{r mapview_cases_perpop2, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
nyt_by_state_w_pop_geo %>%
  filter(continental_48==1) %>%   #Limit to continental 48 so the default zoom is more zoomed in.
  mapview(
    zcol = "cases_cumul_per_pop",
    layer.name = "Cumulative incidence per pop",
    #The palette function is from the viridis package.
    col.regions = viridis_pal(alpha = 1, begin = 0, end = 1, direction = -1, option = "B")
  )
```

### Visualize using a ggplot2 histogram

In addition to this geographic visualization, we may also want a simple aspatial figure summarizing cumulative incidence per population using **ggplot2**. Guidance on ggplot is extensive throughout the internet, for example: <http://www.cookbook-r.com/Graphs/>, and <https://ggplot2.tidyverse.org/>. A detailed discussion of ggplot2 is outside of the current scope. Let's make a quick histogram accepting the ggplot2 defaults to get a sense of the distribution of the cumulative incidence variable.

As with mapview, `ggplot()` can be used after a pipe, so, in the below code the object (`nyt_by_state_w_pop_geo`, after having been modified by the `filter()` function) is passed through to the `ggplot()` function. After we write the initial `ggplot()` function, additional layers are connected with a `+` . In this case, we are just visualizing one variable, `cases_cumul_per_pop`, which we specify using the `aes()` function. We then add a histogram layer using `geom_histogram().` See [here](https://ggplot2.tidyverse.org/reference/geom_histogram.html) for more options to customize histograms.

In subsequent figures, we specify two variables in the `aes(x=xvar, y=var)` function and use different types of layers, e.g., `geom_line()`. Other types of layers include `geom_point()` for a scatterplot or `geom_bar` for a bar chart--and [lots more](https://ggplot2.tidyverse.org/index.html).

```{r gghisto, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
nyt_by_state_w_pop_geo %>%
  filter(continental_48==1) %>%   #Limit to continental 48 so the default zoom is more zoomed in.
  ggplot(aes(x=cases_cumul_per_pop))+
  geom_histogram() 
```

## Link the daily NYT COVID-19 data with the population data to analyze temporal trends in incidence by state

In the above analysis, we collapsed over time. We might also be interested in time trends. In this section, we'll keep the data in its daily format and link in the population data again.

Sometimes, there can be computer-processing benefits to dropping the geometry column, as the geometry column can make the file size quite a bit bigger. Since the daily data are thousands of observations, linking each day's data with a corresponding geometry might be slow to work with. Let's create a new state-population dataset without the geometry column before we link the population data to the daily COVID-19 data. Drop the geometry using the `st_set_geometry(NULL)` function from **sf**.

```{r dropgeo, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
pop_by_state_wrangle_nogeo = pop_by_state_wrangle_geo %>% 
  st_set_geometry(NULL)
```

Now, link the state-level population data to the daily COVID-19 data. Recall, the key variable for our `left_jon()` is 'state'. Calculate daily cumulative incidence of each outcome per population and the daily incidence rate of each outcome.

```{r dailycovidpop, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
nyt_state_pop_wrangle = nyt_state_wrangle %>% 
  left_join(pop_by_state_wrangle_nogeo, by = "state") %>% 
  mutate(
    cases_cumul_per_pop = cases_cumul/population,
    deaths_cumul_per_pop = deaths_cumul/population,
    cases_incident_per_pop = cases_incident/population,
    deaths_incident_per_pop = deaths_incident/population 
  )
```

### Visualize daily cumulative incidence per population for the continental 48 states using ggplot2.

Unlike the above histogram where we were only visualizing one variable, here, we're visualizing `cases_cumul_per_pop` with respect to `date`, so in the `ggplot()` function, we write `aes(x=date, y=cases_cumul_per_pop)`. Additional options are described in the comments below.

```{r statelinegraphs, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
nyt_state_pop_wrangle %>% 
  filter(continental_48==1) %>%   
  ggplot( aes(x=date, y=cases_cumul_per_pop))+ #define the variables to be visualized.
  geom_line(aes(color = state)) + #color by state
  ylab("Cumulative COVID-19\n incidence\n per population") + #add a label to the y-axis
  xlab("Date")  + #add a label to the x-axis
  theme_bw() +  #convert to a black-and-white theme
  theme(legend.position="bottom") + #put the legend on the bottom of the chart.
  scale_x_date(date_breaks = "months" , date_labels = "%b-%y") + 
  theme(axis.text.x = element_text(angle = 90)) #rotate text on x-axis 90 degrees
```



Recall incidence rate is the number of **new** cases per population per time period (day, here). We calculated it at the beginning of the code by subtracting the case count in a state on one day from the corresponding count from that state the day before.

```{r statelinegraphs2, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
nyt_state_pop_wrangle %>% 
  filter(continental_48==1) %>%   
  ggplot( aes(x=date, y=cases_incident_per_pop))+
  geom_line(aes(color = state)) +
  scale_x_date(date_breaks = "months" , date_labels = "%b-%y") +
  ylab("COVID-19\n incidence rate\n per population") +
  xlab("Date")  +
  theme_bw() +
  theme(legend.position="bottom") +
  theme(axis.text.x = element_text(angle = 90))
```

We could make those two charts prettier and easier to read, but graphing the two makes clear the distinction between the two measures (cumulative incidence vs incidence rate). Cumulative incidence only goes up, as expected, while incidence rate goes up and down.

One way to make the state-stratified data easier to read would be to break up the chart by group using a [facet_wrap()](https://ggplot2.tidyverse.org/reference/facet_wrap.html).

### Exercise 2: Use facet_wrap to separate plots into groups.

**Exercise:** Divide the states into regions (e.g., using the classification here, and create line charts of cumulative incidence and incidence rate by state by region.

In April 2021, there was a [reported spike in cases in Michigan](https://www.nytimes.com/2021/04/01/us/michigan-covid-outbreak.html). Let's explore that by subsetting to the Great Lakes region, as we did [previously](https://michaeldgarber.github.io/teach-r/1-dplyr-nyt-covid.html). We can also subset to the year 2021 to further zoom into the relevant time period.

Note that, as we did above before our `mapview()` function, it is useful to pipe all of the data wrangling steps into the `ggplot2()` function instead of creating a new tibble, especially if you don't think you'll use that tibble for anything else. We do that below.

The spike in Michigan is indeed visible. This figure also illustrates how unstable and noisy daily values are.

```{r greatlakes1, eval=TRUE, echo=TRUE, warning=FALSE, message=TRUE}
nyt_state_pop_wrangle %>% 
  mutate(
    great_lakes = case_when(
      state == "Illinois" |
      state == "Indiana" |
      state == "Michigan" |
      state == "Minnesota" |
      state == "New York" |
      state == "Ohio" |
      state == "Pennsylvania" |
      state == "Wisconsin" ~ 1,
      TRUE ~ 0)) %>% 
  filter(great_lakes==1) %>% 
  filter(year == 2021) %>% 
  #Pipe the tibble into the ggplot function:
  ggplot( aes(x=date, y=cases_incident_per_pop))+
  geom_line(aes(color = state)) +
  ylab("COVID-19\n incidence rate\n per population") +
  xlab("Date")  +
  theme_bw() +
  scale_x_date(date_breaks = "months" , date_labels = "%b-%y") +
  theme(legend.position="bottom") +
  theme(axis.text.x = element_text(angle = 90))
```

### Exercise 3: Calculate and graph weekly incidence rate

**Exercise:** Weekly incidence rate is probably a more stable measure. How would we calculate that? Calculate weekly incidence rate and graph both overall (contiguous 48) and by state.

